Configurations:

general:
  device: cuda
  seed: 100
  conv_mode: patches
  deterministic: false
  double_fp: false
  loss_reduction_func: sum
  sparse_alpha: true
  save_adv_example: false
  precompile_jit: false
  complete_verifier: bab
  enable_incomplete_verification: true
  csv_name: instances.csv
  results_file: out.txt
  root_path: ../../vnncomp2022_benchmarks/benchmarks/sri_resnet_b
model:
  name: null
  path: null
  onnx_path: null
  onnx_path_prefix: ''
  cache_onnx_conversion: false
  onnx_quirks: '''Reshape'': ''fix_batch_size'': True'
  input_shape: null
  onnx_loader: default_onnx_and_vnnlib_loader
  onnx_optimization_flags: none
data:
  start: 4
  end: 5
  select_instance: null
  num_outputs: 10
  mean: 0.0
  std: 1.0
  pkl_path: null
  dataset: null
  data_filter_path: null
  data_idx_file: null
specification:
  type: lp
  robustness_type: verified-acc
  norm: .inf
  epsilon: null
  vnnlib_path: null
  vnnlib_path_prefix: ''
solver:
  batch_size: 1024
  min_batch_size_ratio: 0.1
  use_float64_in_last_iteration: false
  early_stop_patience: 10
  start_save_best: 0.5
  bound_prop_method: alpha-crown
  init_bound_prop_method: same
  prune_after_crown: true
  crown:
    batch_size: 1000000000
    max_crown_size: 1000000000
  alpha-crown:
    alpha: true
    lr_alpha: 0.1
    iteration: 100
    share_slopes: false
    no_joint_opt: false
    lr_decay: 0.98
    full_conv_alpha: true
  beta-crown:
    lr_alpha: 0.01
    lr_beta: 0.05
    lr_decay: 0.98
    optimizer: adam
    iteration: 10
    beta: true
    beta_warmup: true
    enable_opt_interm_bounds: false
    all_node_split_LP: false
  forward:
    refine: false
    dynamic: false
    max_dim: 10000
  intermediate_refinement:
    enabled: false
    batch_size: 10
    opt_coeffs: false
    opt_bias: false
    lr: 0.05
    layers: [-1]
    max_domains: 1000
  multi_class:
    label_batch_size: 32
    skip_with_refined_bound: true
  mip:
    parallel_solvers: null
    solver_threads: 1
    refine_neuron_timeout: 15
    refine_neuron_time_percentage: 0.8
    early_stop: true
    adv_warmup: true
    mip_solver: gurobi
bab:
  initial_max_domains: 100
  max_domains: .inf
  decision_thresh: 0
  timeout: 360
  timeout_scale: 1
  override_timeout: null
  get_upper_bound: false
  dfs_percent: 0.0
  pruning_in_iteration: true
  pruning_in_iteration_ratio: 0.2
  sort_targets: false
  batched_domain_list: true
  optimized_intermediate_layers: ''
  interm_transfer: false
  cut:
    enabled: false
    bab_cut: false
    lp_cut: false
    method: null
    lr: 0.01
    lr_decay: 1.0
    iteration: 100
    bab_iteration: -1
    early_stop_patience: -1
    lr_beta: 0.02
    number_cuts: 50
    topk_cuts_in_filter: 100
    batch_size_primal: 100
    max_num: 1000000000
    patches_cut: false
    cplex_cuts: false
    cplex_cuts_wait: 0
    cplex_cuts_revpickup: true
    cut_reference_bounds: true
    fix_intermediate_bounds: false
    _tmp_cuts: null
    fixed_cuts: false
    add_implied_cuts: false
    add_input_cuts: false
  branching:
    method: kfsb-intercept-only
    candidates: 10
    reduceop: max
    sb_coeff_thresh: 0.001
    branching_input_and_activation: false
    branching_input_and_activation_order: [input, relu]
    branching_input_iterations: 30
    branching_relu_iterations: 50
    sort_domain_interval: -1
    input_split:
      enable: false
      enhanced_bound_prop_method: alpha-crown
      enhanced_branching_method: naive
      enhanced_bound_patience: 100000000.0
      attack_patience: 100000000.0
      adv_check: 0
  attack:
    enabled: false
    beam_candidates: 8
    beam_depth: 7
    max_dive_fix_ratio: 0.8
    min_local_free_ratio: 0.2
    mip_start_iteration: 5
    mip_timeout: 30.0
    adv_pool_threshold: null
    refined_mip_attacker: false
    refined_batch_size: null
attack:
  pgd_order: middle
  pgd_steps: 100
  pgd_restarts: 30
  pgd_early_stop: true
  pgd_lr_decay: 0.99
  pgd_alpha: auto
  pgd_loss_mode: null
  enable_mip_attack: false
  cex_path: ./test_cex.txt
  attack_mode: PGD
  gama_lambda: 10.0
  gama_decay: 0.9
  check_clean: false
  input_split:
    pgd_steps: 100
    pgd_restarts: 30
    pgd_alpha: auto
  input_split_enhanced:
    pgd_steps: 200
    pgd_restarts: 5000000
    pgd_alpha: auto
  input_split_check_adv:
    pgd_steps: 5
    pgd_restarts: 5
    pgd_alpha: auto
debug:
  lp_test: null

Experiments at Sun Jan 15 22:12:46 2023 on diablo.cs.ucla.edu
customized start/end sample from instance 4 to 5 in instances.csv
Internal results will be saved to a-b-crown_[instances]_start=4_end=5_iter=10_b=1024_timeout=360_branching=kfsb-intercept-only-max-10_lra-init=0.1_lra=0.01_lrb=0.05_PGD=middle_cplex_cuts=False_initial_max_domains=100.npz.

 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% idx: 0, vnnlib ID: 4 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Using onnx onnx/resnet_3b2_bn_mixup_ssadv_4.0_bs128_lr-1_v2.onnx
Using vnnlib vnnlib/cifar10_spec_idx_3684_eps_0.00350.vnnlib
Precompiled vnnlib file found at ../../vnncomp2022_benchmarks/benchmarks/sri_resnet_b/vnnlib/cifar10_spec_idx_3684_eps_0.00350.vnnlib.compiled
Loading onnx ../../vnncomp2022_benchmarks/benchmarks/sri_resnet_b/onnx/resnet_3b2_bn_mixup_ssadv_4.0_bs128_lr-1_v2.onnx wih quirks {'Reshape': {'fix_batch_size': True}}
Enabling quirks for Reshape operation: fix the first dimension shape to be -1 to support batchsize != 1.
input shape torch.Size([1, 128, 2, 2]), new shape is tensor([ -1, 512]).
Model prediction is: tensor([[-5.11146009e-01, -2.87331522e-01, -5.75531423e-01, -7.01412082e-01,
          1.31808221e-03, -9.98764277e-01,  1.13037780e-01,  2.18221277e-01,
         -1.90814406e-01,  2.93256187e+00]], device='cuda:0')
layer /51 using sparse-features alpha with shape [92]; unstable size 92; total size 4096 (torch.Size([1, 16, 16, 16]))
layer /51 start_node /input.4 using sparse-spec alpha with unstable size 109 total_size 2048 output_shape (32, 8, 8)
layer /51 start_node /56 using sparse-spec alpha with unstable size 124 total_size 2048 output_shape (32, 8, 8)
layer /51 start_node /input.12 using sparse-spec alpha with unstable size 129 total_size 1024 output_shape (64, 4, 4)
layer /51 start_node /62 using sparse-spec alpha with unstable size 183 total_size 1024 output_shape torch.Size([64, 4, 4])
layer /51 start_node /input.20 using sparse-spec alpha with unstable size 164 total_size 512 output_shape torch.Size([128, 2, 2])
layer /51 start_node /68 using sparse-spec alpha with unstable size 288 total_size 512 output_shape torch.Size([128, 2, 2])
layer /51 start_node /input.24 using sparse-spec alpha with unstable size 51 total_size 100 output_shape torch.Size([100])
layer /51 start_node /80 using full alpha with unstable size None total_size 9 output_shape 9
layer /54 using sparse-features alpha with shape [109]; unstable size 109; total size 2048 (torch.Size([1, 32, 8, 8]))
layer /54 start_node /56 using sparse-spec alpha with unstable size 124 total_size 2048 output_shape (32, 8, 8)
layer /54 start_node /input.12 using sparse-spec alpha with unstable size 129 total_size 1024 output_shape (64, 4, 4)
layer /54 start_node /62 using sparse-spec alpha with unstable size 183 total_size 1024 output_shape torch.Size([64, 4, 4])
layer /54 start_node /input.20 using sparse-spec alpha with unstable size 164 total_size 512 output_shape torch.Size([128, 2, 2])
layer /54 start_node /68 using sparse-spec alpha with unstable size 288 total_size 512 output_shape torch.Size([128, 2, 2])
layer /54 start_node /input.24 using sparse-spec alpha with unstable size 51 total_size 100 output_shape torch.Size([100])
layer /54 start_node /80 using full alpha with unstable size None total_size 9 output_shape 9
layer /input.8 using sparse-features alpha with shape [124]; unstable size 124; total size 2048 (torch.Size([1, 32, 8, 8]))
layer /input.8 start_node /input.12 using sparse-spec alpha with unstable size 129 total_size 1024 output_shape (64, 4, 4)
layer /input.8 start_node /62 using sparse-spec alpha with unstable size 183 total_size 1024 output_shape (64, 4, 4)
layer /input.8 start_node /input.20 using sparse-spec alpha with unstable size 164 total_size 512 output_shape torch.Size([128, 2, 2])
layer /input.8 start_node /68 using sparse-spec alpha with unstable size 288 total_size 512 output_shape torch.Size([128, 2, 2])
layer /input.8 start_node /input.24 using sparse-spec alpha with unstable size 51 total_size 100 output_shape torch.Size([100])
layer /input.8 start_node /80 using full alpha with unstable size None total_size 9 output_shape 9
layer /60 using sparse-features alpha with shape [129]; unstable size 129; total size 1024 (torch.Size([1, 64, 4, 4]))
layer /60 start_node /62 using sparse-spec alpha with unstable size 183 total_size 1024 output_shape (64, 4, 4)
layer /60 start_node /input.20 using sparse-spec alpha with unstable size 164 total_size 512 output_shape torch.Size([128, 2, 2])
layer /60 start_node /68 using sparse-spec alpha with unstable size 288 total_size 512 output_shape torch.Size([128, 2, 2])
layer /60 start_node /input.24 using sparse-spec alpha with unstable size 51 total_size 100 output_shape torch.Size([100])
layer /60 start_node /80 using full alpha with unstable size None total_size 9 output_shape 9
layer /input.16 using sparse-features alpha with shape [183]; unstable size 183; total size 1024 (torch.Size([1, 64, 4, 4]))
layer /input.16 start_node /input.20 using sparse-spec alpha with unstable size 164 total_size 512 output_shape (128, 2, 2)
layer /input.16 start_node /68 using sparse-spec alpha with unstable size 288 total_size 512 output_shape torch.Size([128, 2, 2])
layer /input.16 start_node /input.24 using sparse-spec alpha with unstable size 51 total_size 100 output_shape torch.Size([100])
layer /input.16 start_node /80 using full alpha with unstable size None total_size 9 output_shape 9
layer /66 using sparse-features alpha with shape [164]; unstable size 164; total size 512 (torch.Size([1, 128, 2, 2]))
layer /66 start_node /68 using sparse-spec alpha with unstable size 288 total_size 512 output_shape torch.Size([128, 2, 2])
layer /66 start_node /input.24 using sparse-spec alpha with unstable size 51 total_size 100 output_shape torch.Size([100])
layer /66 start_node /80 using full alpha with unstable size None total_size 9 output_shape 9
layer /69 using sparse-features alpha with shape [288]; unstable size 288; total size 512 (torch.Size([1, 128, 2, 2]))
layer /69 start_node /input.24 using sparse-spec alpha with unstable size 51 total_size 100 output_shape torch.Size([100])
layer /69 start_node /80 using full alpha with unstable size None total_size 9 output_shape 9
layer /79 using sparse-features alpha with shape [51]; unstable size 51; total size 100 (torch.Size([1, 100]))
layer /79 start_node /80 using full alpha with unstable size None total_size 9 output_shape 9
Optimizable variables initialized.
initial CROWN bounds: tensor([[-0.49703509, -1.41750455, -0.21669674, -0.26954985, -0.98067737,
          0.31867206, -1.53754401, -1.52032316, -1.12586951]], device='cuda:0') None
Remain 8 labels need to be attacked.
Attack parameters: initialization=uniform, steps=100, restarts=30, alpha=0.004388190805912018, initialization=uniform, GAMA=False
Model output of first 5 examples:
 tensor([[-5.11146009e-01, -2.87331522e-01, -5.75531423e-01, -7.01412082e-01,
          1.31808221e-03, -9.98764277e-01,  1.13037780e-01,  2.18221277e-01,
         -1.90814406e-01,  2.93256187e+00]], device='cuda:0')
Adv example prediction (first 2 examples and 2 restarts):
 tensor([[[-4.64496553e-01, -2.41121888e-01, -6.11258268e-01, -7.26925790e-01,
          -1.93803757e-03, -1.00051010e+00,  9.45717394e-02,  3.65668416e-01,
          -2.32504681e-01,  2.81865120e+00],
         [-4.64496553e-01, -2.41121888e-01, -6.11258268e-01, -7.26925790e-01,
          -1.93803757e-03, -1.00051010e+00,  9.45717394e-02,  3.65668416e-01,
          -2.32504681e-01,  2.81865120e+00]]], device='cuda:0')
PGD attack margin (first 2 examles and 10 specs):
 tensor([[[3.28314781, 3.05977297, 3.42990947, 3.54557705, 2.82058930,
          2.72407937, 2.45298290, 3.05115581]]], device='cuda:0')
number of violation:  0
Attack finished in 1.1376 seconds.
PGD attack failed
prune_after_crown optimization in use: original label size = 9 pruned label size = 8
best_l after optimization: -1.9923344850540161 with beta sum per layer: []
alpha/beta optimization time: 23.465352535247803
initial alpha-CROWN bounds: tensor([[ 0.06396627, -0.60740447,  0.49080074,  0.48686814, -0.32611167,
                 inf, -0.80300426, -0.76234794, -0.53510129]], device='cuda:0')
Worst class: (+ rhs) -0.803004264831543
Total VNNLIB file length: 9, max property batch size: 100, total number of batches: 1
lA shape: [torch.Size([1, 9, 16, 16, 16]), torch.Size([1, 9, 32, 8, 8]), torch.Size([1, 9, 32, 8, 8]), torch.Size([1, 9, 64, 4, 4]), torch.Size([1, 9, 64, 4, 4]), torch.Size([1, 9, 128, 2, 2]), torch.Size([1, 9, 128, 2, 2]), torch.Size([1, 9, 100])]

Properties batch 0, size 9
Remaining timeout: 269.068656206131
##### Instance 0 first 10 spec matrices: [[[-1.  0.  0.  0.  0.  0.  0.  0.  0.  1.]]

 [[ 0. -1.  0.  0.  0.  0.  0.  0.  0.  1.]]

 [[ 0.  0. -1.  0.  0.  0.  0.  0.  0.  1.]]

 [[ 0.  0.  0. -1.  0.  0.  0.  0.  0.  1.]]

 [[ 0.  0.  0.  0. -1.  0.  0.  0.  0.  1.]]

 [[ 0.  0.  0.  0.  0. -1.  0.  0.  0.  1.]]

 [[ 0.  0.  0.  0.  0.  0. -1.  0.  0.  1.]]

 [[ 0.  0.  0.  0.  0.  0.  0. -1.  0.  1.]]

 [[ 0.  0.  0.  0.  0.  0.  0.  0. -1.  1.]]]
thresholds: [0. 0. 0. 0. 0. 0. 0. 0. 0.] ######
Initial alpha-CROWN verified for spec index [0 2 3 5] with bound tensor([0.06396627, 0.49080074, 0.48686814,        inf], device='cuda:0').
Remaining spec index [1 4 6 7 8] with bounds tensor([[-0.60740447],
        [-0.32611167],
        [-0.80300426],
        [-0.76234794],
        [-0.53510129]], device='cuda:0') need to verify.
Model prediction is: tensor([-5.11146009e-01, -2.87331522e-01, -5.75531423e-01, -7.01412082e-01,
         1.31808221e-03, -9.98764277e-01,  1.13037780e-01,  2.18221277e-01,
        -1.90814406e-01,  2.93256187e+00], device='cuda:0')
build_the_model_with_refined_bounds batch [0/1]
setting alpha for layer /51 start_node /80 with alignment adjustment
setting alpha for layer /54 start_node /80 with alignment adjustment
setting alpha for layer /input.8 start_node /80 with alignment adjustment
setting alpha for layer /60 start_node /80 with alignment adjustment
setting alpha for layer /input.16 start_node /80 with alignment adjustment
setting alpha for layer /66 start_node /80 with alignment adjustment
setting alpha for layer /69 start_node /80 with alignment adjustment
setting alpha for layer /79 start_node /80 with alignment adjustment
all slope initialized
directly get lb and ub from refined bounds
lA shapes: [torch.Size([1, 5, 16, 16, 16]), torch.Size([1, 5, 32, 8, 8]), torch.Size([1, 5, 32, 8, 8]), torch.Size([1, 5, 64, 4, 4]), torch.Size([1, 5, 64, 4, 4]), torch.Size([1, 5, 128, 2, 2]), torch.Size([1, 5, 128, 2, 2]), torch.Size([1, 5, 100])]
c shape: torch.Size([5, 1, 10])
alpha-CROWN with fixed intermediate bounds: tensor([[-0.60740447],
        [-0.32611167],
        [-0.80300426],
        [-0.76234794],
        [-0.53510129]], device='cuda:0') tensor([[inf],
        [inf],
        [inf],
        [inf],
        [inf]], device='cuda:0')
Keeping slopes for these layers: ['/80']
Keeping slopes for these layers: ['/80']
layer 0 name BoundConv(name="/input") size torch.Size([4096]) unstable 92
layer 1 name BoundConv(name="/input.4") size torch.Size([2048]) unstable 109
layer 2 name BoundAdd(name="/56") size torch.Size([2048]) unstable 122
layer 3 name BoundConv(name="/input.12") size torch.Size([1024]) unstable 127
layer 4 name BoundAdd(name="/62") size torch.Size([1024]) unstable 181
layer 5 name BoundConv(name="/input.20") size torch.Size([512]) unstable 158
layer 6 name BoundAdd(name="/68") size torch.Size([512]) unstable 281
layer 7 name BoundLinear(name="/input.24") size torch.Size([100]) unstable 49
-----------------
# of unstable neurons: 1119
-----------------

batch:  torch.Size([5, 16, 16, 16]) pre split depth:  4
post split depth:  4
splitting decisions: 
split level 0: [7, 20] [7, 6] [7, 77] [7, 69] [7, 64] 
split level 1: [7, 40] [7, 69] [7, 69] [7, 25] [7, 25] 
split level 2: [7, 3] [7, 43] [7, 40] [7, 52] [7, 30] 
split level 3: [7, 50] [7, 25] [7, 50] [7, 92] [7, 50] 
pruning_in_iteration open status: True
ratio of positive domain = 76 / 80 = 0.95
pruning-in-iteration extra time:/home/zhouxingshi/onnx2pytorch/onnx2pytorch/convert/layer.py:30: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_numpy.cpp:178.)
  layer.weight.data = torch.from_numpy(numpy_helper.to_array(weight))
/home/zhouxingshi/onnx2pytorch/onnx2pytorch/convert/model.py:154: UserWarning: Using experimental implementation that allows 'batch_size > 1'.Batchnorm layers could potentially produce false outputs.
  "Using experimental implementation that allows 'batch_size > 1'."
/home/zhouxingshi/onnx2pytorch/onnx2pytorch/operations/reshape.py:45: UserWarning: __rfloordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').
  inferred_shape = prod(input.shape[1:]) // incomplete_shape
/home/zhouxingshi/onnx2pytorch/onnx2pytorch/utils.py:21: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  return value.ndim == 0 or value.shape == torch.Size([1])
/home/zhouxingshi/onnx2pytorch/onnx2pytorch/operations/add.py:34: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  elif all(x == 1 for x in input[0].shape):
/home/zhouxingshi/onnx2pytorch/onnx2pytorch/operations/reshape.py:36: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!
  if (shape[0] == 1 and (len(shape) == 4 or len(shape) == 2)
/home/zhouxingshi/onnx2pytorch/onnx2pytorch/operations/reshape.py:55: TracerWarning: Iterating over a tensor might cause the trace to be incorrect. Passing a tensor of different shape won't change the number of iterations executed (and might lead to errors or silently give incorrect results).
  shape = [x if x != 0 else input.size(i) for i, x in enumerate(shape)]
 0.009032964706420898
Time: prepare 0.0155    beta_bound 0.6797    bound 0.6798    transfer 0.0013    finalize 0.0092    func 0.7059    
Accumulated time: func 0.7059    prepare 0.0268    bound 0.6798    beta_bound 0.6797    transfer 0.0013    finalize 0.0092    
batch bounding time:  0.7060108184814453
Current worst splitting domains lb-rhs (depth):
-0.19724 (4), -0.18671 (4), -0.15156 (4), -0.06766 (4), 
length of domains: 4
Time: pickout 0.0024    decision 0.5411    solve 0.7193    add 0.0033    
Accumulated time: pickout 0.0024    decision 0.5411    solve 0.7193    add 0.0033    
Current (lb-rhs): -0.19724203646183014
4 domains visited
Cumulative time: 1.3537955284118652

batch:  torch.Size([4, 16, 16, 16]) pre split depth:  4
post split depth:  4
splitting decisions: 
split level 0: [7, 3] [7, 96] [7, 3] [7, 96] 
split level 1: [7, 40] [7, 25] [7, 40] [7, 25] 
split level 2: [7, 50] [7, 10] [7, 50] [7, 10] 
split level 3: [7, 30] [7, 29] [7, 30] [6, 43] 

all verified at 0th iter
pruning_in_iteration open status: False
ratio of positive domain = 64 / 64 = 1.0
pruning-in-iteration extra time: 0.00010538101196289062
Time: prepare 0.0153    beta_bound 0.0165    bound 0.0166    transfer 0.0007    finalize 0.0080    func 0.0406    
Accumulated time: func 0.7465    prepare 0.0532    bound 0.6964    beta_bound 0.6962    transfer 0.0020    finalize 0.0172    
batch bounding time:  0.04072165489196777
length of domains: 0
Time: pickout 0.0023    decision 0.1377    solve 0.0517    add 0.0016    
Accumulated time: pickout 0.0047    decision 0.6788    solve 0.7710    add 0.0048    
No domains left, verification finished!
Current (lb-rhs): 1.0000000116860974e-07
4 domains visited
Cumulative time: 1.5475854873657227

Result: safe in 32.8425 seconds
############# Summary #############
Final verified acc: 100.0% (total 1 examples)
Problem instances count: 1 , total verified (safe/unsat): 1 , total falsified (unsafe/sat): 0 , timeout: 0
mean time for ALL instances (total 1):32.842168450263735, max time: 32.84249687194824
mean time for verified SAFE instances(total 1): 32.84249687194824, max time: 32.84249687194824
safe (total 1), index: [0]
